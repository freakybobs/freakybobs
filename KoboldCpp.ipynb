{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyM8fpNJL+inQH2+qNKY4iDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freakybobs/freakybobs/blob/master/KoboldCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b> [1]  If you play on mobile, tap this to open music player and play the white noise to keep tab running in the background. or google will kill your api\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AXeQ3fyGWn35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF0nJ1VzRE8C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title <b>v--Select model and then click this to start Koboldcpp (takes around 2-3 min for link to be generated)</b> {\"display-mode\":\"form\"}\n",
        "import os\n",
        "import time\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU because you used it to often recently, this can take a few hours before they let you back in. Try again later or subscribe to Colab Pro for immediate access. (or change email if you are really desperate)⚠️\")\n",
        "import time\n",
        "from google.colab import runtime\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import re\n",
        "\n",
        "# @markdown ### DO NOT CLOSE THIS TAB WHILE USING, GOOGLE ANTI AFK WILL KILL API\n",
        "\n",
        "Model = \"Llama3-Mullein(24B)\"  # @param [\"daybreak-kunoichi(7B)\",\"Llama3-Lumimaid(8B)\",\"Sunfall-NemoMix(12B)\",\"Marinara-Nemomix(12B)\",\"Llama3-Mullein(24B)\"]{allow-input: true}\n",
        "Layers = 99 #@param [99]{allow-input: true}\n",
        "ContextSize = 4096 #@param [4096,8192] {allow-input: true}\n",
        "FlashAttention = True #@param {type:\"boolean\"}\n",
        "MakeLocalTunnelFallback = True #@param {type:\"boolean\"}\n",
        "force_redownload_kobold = True # @param {type:\"boolean\"}\n",
        "\n",
        "modellink = ''\n",
        "\n",
        "import os\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "\n",
        "if FlashAttention:\n",
        "  FACommand = \"--flashattention\"\n",
        "else:\n",
        "  FACommand = \"\"\n",
        "\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "clear_output()\n",
        "if \"https\" in Model:\n",
        "  modellink = Model\n",
        "else:\n",
        "  match Model:\n",
        "    case \"daybreak-kunoichi(7B)\" :\n",
        "        modellink = \"https://huggingface.co/crestf411/daybreak-kunoichi-2dpo-7b-gguf/resolve/main/daybreak-kunoichi-2dpo-7b-q4_k_m.gguf\"\n",
        "    case \"Llama3-Lumimaid(8B)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix/resolve/main/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q4_K_S-imat.gguf\"\n",
        "    case \"Sunfall-NemoMix(12B)\" :\n",
        "        modellink = \"https://huggingface.co/Vdr1/Sunfall-NemoMix-Unleashed-12B-v0.6.1-GGUF-IQ/resolve/main/Sunfall-NemoMix-Unleashed-12B-v0.6.1-Q4_K_M.gguf\"\n",
        "    case \"Marinara-Nemomix(12B)\" :\n",
        "        modellink = \"https://huggingface.co/featherless-ai-quants/MarinaraSpaghetti-Nemomix-v4.0-12B-GGUF/resolve/main/MarinaraSpaghetti-Nemomix-v4.0-12B-Q4_K_S.gguf\"\n",
        "    case \"Llama3-Mullein(24B)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Llama3-24B-Mullein-v1-GGUF/resolve/main/Llama3-24B-Mullein-v1.Q4_K_S.gguf\"\n",
        "\n",
        "modelname = modellink.split('/')[-1].split('.')[0]\n",
        "matched = re.search(r'/([^/]+)-GGUF/', modellink)\n",
        "remodelname = (\"koboldcpp/model\")\n",
        "if matched:\n",
        "    remodelname = f\"koboldcpp/{matched.group(1)}\"\n",
        "isModelExist = os.path.isfile('model.txt')\n",
        "oldmodel = \"\"\n",
        "if(isModelExist):\n",
        "  f = open(\"model.txt\", \"r\")\n",
        "  oldmodel = f.read()\n",
        "  print(f'oldmodel = {oldmodel}')\n",
        "\n",
        "if(oldmodel != modellink or isModelExist == False):\n",
        "  print('Download: '+ modellink)\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M -o model.gguf $modellink\n",
        "  # !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $modellink -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "  with open('model.txt', 'w') as f:\n",
        "      f.write(modellink)\n",
        "clear_output()\n",
        "if MakeLocalTunnelFallback:\n",
        "  import urllib\n",
        "  print(\"Trying to use LocalTunnel as a fallback tunnel (not so good)...\")\n",
        "  ltpw = urllib.request.urlopen('https://loca.lt/mytunnelpassword').read().decode('utf8').strip(\"\\n\")\n",
        "  !nohup npx --yes localtunnel --port 5001 > lt.log 2>&1 &\n",
        "  !sleep 8\n",
        "  print(\"=================\")\n",
        "  print(\"(LocalTunnel Results)\")\n",
        "  !cat lt.log\n",
        "  print(f\"Please open the above link, and input the password '{ltpw}'\\nYour KoboldCpp will start shortly...\")\n",
        "  print(\"=================\")\n",
        "  !sleep 10\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --chatcompletionsadapter AutoGuess --multiuser --gpulayers $Layers --contextsize $ContextSize --websearch --quiet --remotetunnel $FACommand"
      ]
    }
  ]
}